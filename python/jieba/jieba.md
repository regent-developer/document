# python jieba

jieba是优秀的中文分词第三方库

## 简介

### 概述

* jieba是优秀的中文分词第三方库

* 中文文本需要通过分词获得单个的词语
* jieba是优秀的中文分词第三方库，需要额外安装           

* jieba库提供三种分词模式，最简单只需掌握一个函数

### jieba分词的原理

* 分词依靠中文词库

* 利用一个中文词库，确定汉字之间的关联概率

* 汉字间概率大的组成词组，形成分词结果

* 除了分词，用户还可以添加自定义的词组
  

## 安装

```shell
pip install jieba
```

## 使用

jieba分词的三种模式

* 精确模式

```python
 # 精确模式：把文本精确的切分开，不存在冗余单词
 jieba.cut(s)
```



* 全模式

```python
# 全模式：把文本中所有可能的词语都扫描出来，有冗余 
jieba.lcut(s,cut_all=True)
```



* 搜索引擎模式

```python
# 搜索引擎模式：在精确模式基础上，对长词再次切分
jieba.lcut_for_search(s) 
```

