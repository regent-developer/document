# 张量

## 张量的基本概念

PyTorch 是一个 Python 深度学习框架，它将数据封装成张量（Tensor）来进行运算。PyTorch 中的张量就是元素为同一种数据类型的多维矩阵。在 PyTorch 中，张量以 "类" 的形式封装起来，对张量的一些运算、处理的方法被封装在类中。

## 张量的创建

### 基本创建方式
* torch.tensor 根据指定数据创建张量
* torch.Tensor 根据形状创建张量, 其也可用来创建指定数据的张量
* torch.IntTensor、torch.FloatTensor、torch.DoubleTensor 创建指定类型的张量

```python
import torch
import numpy as np
import random

# 根据已有数据创建张量
# 创建张量标量
data = torch.tensor(10)

# 使用numpy数组创建张量
data = np.random.randn(2, 3)
data = torch.tensor(data)

# 使用列表创建张量
data = [[10., 20., 30.], [40., 50., 60.]]
data = torch.tensor(data)

# 创建指定形状的张量
# 创建2行3列的张量, 默认 dtype 为 float32
data = torch.Tensor(2, 3)

# 则创建包含指定元素（列表）的张量
data = torch.Tensor([10])
data = torch.Tensor([10, 20])

# 使用具体类型的张量
# 创建2行3列张量（dtype 为 int32 的张量）
data = torch.IntTensor(2, 3)

# 如果传递的元素类型不正确, 则会进行类型转换
data = torch.IntTensor([2.5, 3.3])

data = torch.ShortTensor()  # int16
data = torch.LongTensor()   # int64
data = torch.FloatTensor()  # float32
data = torch.DoubleTensor() # float64
```

### 创建线性和随机张量
* torch.arange 和 torch.linspace 创建线性张量
* torch.random.init_seed 和 torch.random.manual_seed 随机种子设置
* torch.randn 创建随机张量

```python
import torch

# 在指定区间按照步长生成元素 (start, end, step)
data = torch.arange(0, 10, 2)

# 在指定区间按照元素个数生成
data = torch.linspace(0, 11, 10)

# 创建随机张量
data = torch.randn(2, 3)  # 创建2行3列张量

# 随机数种子设置
print('随机数种子:', torch.random.initial_seed())
torch.random.manual_seed(100) # 设置随机种子
print('随机数种子:', torch.random.initial_seed())

```



### 创建01张量

* torch.ones 和 torch.ones_like 创建全1张量

* torch.zeros 和 torch.zeros_like 创建全0张量

* torch.full 和 torch.full_like 创建全为指定值张量

```python
import torch

# 创建指定形状全0张量
data = torch.zeros(2, 3)

# 根据张量形状创建全0张量
data = torch.zeros_like(data)

# 创建指定形状全0张量
data = torch.ones(2, 3)

# 根据张量形状创建全1张量
data = torch.ones_like(data)

# 创建指定形状指定值的张量
data = torch.full([2, 3], 10)

# 根据张量形状创建指定值的张量
data = torch.full_like(data, 20)
```



### 张量元素类型转换

* tensor.type(torch.DoubleTensor)
* tensor.type(torch.ShortTensor)
* tensor.type(torch.IntTensor)
* tensor.type(torch.LongTensor)
* tensor.type(torch.FloatTensor)

* torch.double()
* torch.short()
* torch.int()
* torch.long()
* torch.float()

```python
import torch

data = torch.randn(2, 3)
print(data.dtype)

data = data.type(torch.DoubleTensor)
print(data.dtype)
data = data.type(torch.ShortTensor)
print(data.dtype)
data = data.type(torch.IntTensor)
print(data.dtype)
data = data.type(torch.FloatTensor)
print(data.dtype)
data = data.type(torch.LongTensor)
print(data.dtype)

data = data.double()
print(data.dtype)
data = data.short()
print(data.dtype)
data = data.int()
print(data.dtype)
data = data.long()
print(data.dtype)
data = data.float()
print(data.dtype)
```



## 张量的数值计算



### 张量基本运算

* add
* sub
* mul
* div
* neg
* add_
* sub_
* mul_
* div_
* neg_

**Torch里面所有带"_"的操作，都是in-place的，修改原数据。比如x.add_(y)，x+y的结果会存储到原来的x中。**

```python
import torch
import numpy as np

data = torch.randint(0, 10, [2, 3])
print(data)

# 不修改原数据
new_data = data.add(10) 
print(new_data)
print(data)

# 修改原数据
data.add_(10) 
print(data)
```



### 阿达玛积

阿达玛积指的是矩阵对应位置的元素相乘。

```python
import torch

data1 = torch.tensor([[1, 2], [3, 4]])
data2 = torch.tensor([[5, 6], [7, 8]])

# 方法一
data = torch.mul(data1, data2)

# 方法二
data = data1.mul(data2)

# 方法三
data = data1 * data2

```



### 点积运算

点积运算要求第一个矩阵 shape: (n, m)，第二个矩阵 shape: (m, p), 两个矩阵点积运算 shape 为: (n, p)。

* 运算符 @ 用于进行两个矩阵的点乘运算
* torch.mm 用于进行两个矩阵点乘运算, 要求输入的矩阵为2维
* torch.bmm 用于批量进行矩阵点乘运算, 要求输入的矩阵为3维
* torch.matmul 对进行点乘运算的两矩阵形状没有限定
  * 对于输入都是二维的张量相当于 mm 运算
  * 对于输入都是三维的张量相当于 bmm 运算
  * 对数输入的 shape 不同的张量, 对应的最后几个维度必须符合矩阵运算规则



```python
import torch

data1 = torch.tensor([1, 2], [2, 3], [3, 4])
data2 = torch.tensor([1, 2], [2, 3])

# 方法一
data = data1 @ data2

# 方法二
data = torch.mm(data1, data2)

# 方法三
data = torch.matmul(data1, data2)

# troch.bmm
data1 = torch.randn(3, 4, 5)
data2 = torch.randn(3, 5, 8)

data = torch.bmm(data1, data2)
```



### 指定运算设备

PyTorch 默认会将张量创建在 CPU 控制的内存中, 即: 默认的运算设备为 CPU。我们也可以将张量创建在 GPU 上, 能够利用对于矩阵计算的优势加快模型训练。

将张量移动到 GPU 上的方法:

* 使用 cuda 方法
* 直接在 GPU 上创建张量
* 使用 to 方法指定设备

```python
import torch

# 使用cuda方法
data = torch.tensor([10, 20 ,30])
print('存储设备：', data.device)

# 如果安装的不是GPU版的pytorch或者没有NVIDIA显卡的话，会报错
data = data.cuda()
print('存储设备:', data.device)

data = data.cpu()
print('存储设备:', data.device)
    
# 直接在GPU上创建张量
data = torch.tensor([10, 20, 30], device='cuda:0')
print('存储设备:', data.device)

## 使用cpu函数将张量移动到 cpu 上
data = data.cpu()
print('存储设备:', data.device)
    
# 使用to方法指定设备
data = torch.tensor([10, 20, 30])
print('存储设备:', data.device)

data = data.to('cuda:0')
print('存储设备:', data.device)

# 存储在不同设备的张量不能运算

data1 = torch.tensor([10, 20, 30], device='cuda:0')
data2 = torch.tensor([10, 20, 30])
print(data1.device, data2.device)

data = data1 + data2
print(data)
```



## 张量的类型转换



### 张量转换为numpy数组

使用 tensor.numpy 函数可以将张量转换为 ndarray 数组，但是是共享内存，可以使用 copy 函数避免共享。

```python
import torch

# 使用张量对象中numpy函数进行转换
data_tensor = torch.tensor([2, 3, 4])
data_numpy = data_tensor.numpy()
print('使用张量对象中numpy函数进行转换')
print(data_tensor)
print(data_numpy)

# 修改转换后数组，原数据改变
print('修改转换后数组，原数据改变')
data_numpy[0] = 100
print(data_tensor)
print(data_numpy)

# 使用copy函数进行转换拷贝
data_copy = torch.tensor([2, 3, 4])
data_copy.copy_(data_tensor)

data_numpy  = data_copy.numpy()
print('使用copy函数进行转换拷贝')
print(data_tensor)
print(data_numpy)

# 修改转换后数组，原数据不变
print('修改转换后数组，原数据不变')
data_numpy[0] = 1
print(data_tensor)
print(data_numpy)
```



### numpy数组转化为张量

* 使用 from_numpy 可以将 ndarray 数组转换为 Tensor，默认共享内存，使用 copy 函数避免共享。

* 使用 torch.tensor 可以将 ndarray 数组转换为 Tensor，默认不共享内存。

```python
import torch
import numpy as np

# 使用 from_numpy 函数
data_numpy = np.array([2, 3, 4])

# 浅拷贝
data_tensor = torch.from_numpy(data_numpy)
print(data_tensor)

# 深拷贝
data_copy.copy_(data_numpy)
data_tensor = torch.from_numpy(data_copy)
print(data_tensor)

# 使用 torch.tensor 函数
data_tensor = torch.tensor(data_numpy)
print(data_tensor)
```

### 标量张量和数字的转换

对于只有一个元素的张量，使用 item 方法将该值从张量中提取出来

```python
import torch

# 当张量只包含一个元素时, 可以通过 item 函数提取出该值
data = torch.tensor([10,])
print(data.item())

data = torch.tensor(10)
print(data.item())
```



## 张量拼接

### torch.cat 函数的使用

torch.cat 函数可以将两个张量根据指定的维度拼接起来

```python
import torch

data1 = torch.randint(0, 10, [3, 5, 4])
data2 = torch.randint(0, 10, [3, 5, 4])

# 按0维度拼接
new_data = torch.cat([data1, data2], dim=0)
print(new_data.shape)

# 按1维度拼接
new_data = torch.cat([data1, data2], dim=1)
print(new_data.shape)

# 按2维度拼接
new_data = torch.cat([data1, data2], dim=2)
print(new_data.shape)   

```



### torch.stack 函数的使用

torch.stack 函数可以将两个张量根据指定的维度叠加起来

把多个2维的张量凑成一个3维的张量；多个3维的凑成一个4维的张量…以此类推，也就是在**增加新的维度进行堆叠**



```python
import torch

data1= torch.randint(0, 10, [2, 3])
data2= torch.randint(0, 10, [2, 3])
print(data1)
print(data2)

# 按0维度叠加
new_data = torch.stack([data1, data2], dim=0)
print(new_data.shape)

# 按1维度叠加
new_data = torch.stack([data1, data2], dim=1)
print(new_data.shape)

# 按2维度叠加
new_data = torch.stack([data1, data2], dim=2)
print(new_data.shape)
```



## 张量索引

### 简单行列索引

```python
import torch

data = torch.randint(0, 10, [4, 5])
print(data)

# 简单行索引
print(data[0])

# 简单列索引
print(data[:, 0])
```

### 列表索引

```python
import torch

data = torch.randint(0, 10, [4, 5,3])
print(data)

# 维度需保持一致，否则报错（IndexError: too many indices for tensor of dimension n）
print(data[[0,1], [1,1], [2,2]])

print(data[[0], [:,1], [2]])
```



### 范围索引

```python
import torch

data = torch.randint(0, 10, [4, 5,3])
print(data)

# 第一个维度的前两个，第二个维度的前两个
print(data[:2,:2])
```



### 布尔索引

```python
import torch

data = torch.randint(0, 10, [4, 5])
print(data)

# 第三列大于五的行数据
print(data[data[:, 2] > 5])

```



### 多维索引

```python
import torch

data = torch.randint(0, 10, [4, 5, 2])
print(data)

print(data[0, :, :])

```



## 张量形状

搭建网络模型时，数据都是基于张量形式的表示，网络层与层之间很多都是以不同的 shape 的方式进行表现和运算，需要对张量形状的操作，以更好处理网络各层之间的数据连接。



### reshape函数

reshape 函数可以在保证张量数据不变的前提下改变数据的维度，将其转换成指定的形状。

```python
import torch
import numpy as np

data = torch.tensor([[10, 20, 30], [40, 50, 60]])
print(data)
print(data.shape)
print(data.size())

print(data.data.reshape(1, 6))
print(data.data.reshape(1, 6).shape)


```



### transpose和permute函数

* transpose函数可以实现交换张量形状的指定维度

* permute函数可以一次交换更多的维度

```python
import torch
import numpy as np

data = torch.tensor(np.random.randint(0, 10, [3, 4, 5]))
print(data.size())

# 使用transpose交换维度1和维度2
new_data = torch.transpose(data, 1, 2)
print(new_data.size())

# 多次使用transpose将形状修改为(4,5,3)
new_data = torch.transpose(data, 0, 1)
new_data = torch.transpose(new_data, 1, 2)
print(new_data.size())

# 使用permute将形状修改为(4,5,3)
new_data = torch.permute(data, [1, 2, 0])
print(new_data.size())

```



### view和contigous函数

view 函数也可以用于修改张量的形状，但是其用法比较局限，只能用于存储在整块内存中的张量。在 PyTorch 中，有些张量是由不同的数据块组成的，它们并没有存储在整块的内存中，view 函数无法对这样的张量进行变形处理。

一个张量经过了 transpose 或者 permute 函数的处理之后，就无法使用 view 函数进行形状操作。需要先使用 contiguous 函数转换为整块内存的张量，再使用 view 函数



```python
import torch
import numpy as np

data = torch.tensor([[10, 20, 30], [40, 50, 60]])
print(data.size())

# 使用view函数修改形状
new_data = data.view(3, 2)
print(new_data.shape)

# 判断张量是否使用整块内存
print(data.is_contiguous())  # True

# 使用transpose函数修改形状
new_data = torch.transpose(data, 0, 1)
print(data.is_contiguous())  # False

# 先使用contiguous函数转换为整块内存的张量，再使用 view 函数
new_data = new_data.contiguous().view(2, 3)
print(data.is_contiguous())  # True
```



### squeeze和unsqueeze函数

* squeeze 函数：用删除 shape 为 1 的维度
* unsqueeze函数： 在每个维度添加 1, 以增加数据的形状

```python
import torch
import numpy as np

data = torch.tensor(np.random.randint(0, 10, [1, 3, 1, 5]))
print(data.shape)

# 去掉值为1的维度
new_data = data.squeeze()
print(new_data.shape)

# 去掉指定位置为1的维度
new_data = data.squeeze(2)
print(new_data.shape)

# 在2维度增加一个维度
new_data = data.unsqueeze(-1)
print(new_data.shape)
```



## 张量运算

* 平均值：tensor.mean()
* 和：tensor.sum()
* 平方：tensor.pow(2)
* 平方根：tensor.sqrt()
* 指数：tensor.exp()
* 对数：tensor.log()
* .....

```python
import torch
data = torch.randint(0, 10, [2, 3], dtype=torch.float64)
print(data)

# 平均值
print(data.mean())

# 和
print(data.sum())

# 平方
print(data.pow(2))

# 平方根
print(data.sqrt())

# 指数
print(data.exp()) # e^n 次方

# 对数
print(data.log()) # 以e为底
```



## 自动微分模块

自动微分（Autograd）模块对张量做了进一步的封装，具有自动求导功能。

自动微分模块是构成神经网络训练的必要模块，在神经网络的反向传播过程中，Autograd 模块基于正向计算的结果对当前的参数进行微分计算，从而实现网络权重参数的更新。

### tensor说明

| Tensor属性      | 作用                                                         |
| --------------- | ------------------------------------------------------------ |
| device          | 该节点运行的设备环境，即CPU/GPU                              |
| `requires_grad` | 自动微分机是否需要对该节点求导，缺省为False                  |
| grad            | 输出节点对该节点的梯度，缺省为None                           |
| grad_fn         | 中间计算节点关于全体输入节点的映射，记录了前向传播经过的操作。叶节点为None |
| is_leaf         | 该节点是否为叶节点                                           |

> 将Tensor的requires_grad属性设置为True时，Pytorch的torch.autograd会**自动地追踪它的计算轨迹**，**当需要计算微分的时候，只需要对最终计算结果的Tensor调用backward方法**，**中间所有计算节点的微分就会被保存在grad属性中**



### 梯度基本计算

使用 backward 方法、grad 属性来实现梯度的计算和访问。

```python
import torch

# 单标量梯度的计算
# 定义需要求导的张量，张量的值类型必须是浮点类型
x = torch.tensor(10, requires_grad=True, dtype=torch.float64)
print(x)

# 变量经过中间运算
# 微分是 f'(x) = 2x
f = x ** 2 + 20

# 自动微分
f.backward()

# 打印x变量的梯度
# backward函数计算的梯度值会存储在张量的grad变量中
print(x.grad)
print(x) 

# 单向量梯度的计算
# 定义需要求导张量
x = torch.tensor([10, 20, 30, 40], requires_grad=True, dtype=torch.float64)
   
# 变量经过中间运算
# 微分是 f1'(x) = 2x
f1 = x ** 2 + 20
 
# 由于求导的结果必须是标量，而 f 的结果是: tensor([120., 420.])，不能直接自动微分。需要将结果计算为标量才能进行计算
f2 = f1.mean()

# 自动微分
f2.backward()

# 打印 x 变量的梯度
print(x.grad)

# 多标量梯度计算
# 定义需要计算梯度的张量
x1 = torch.tensor(10, requires_grad=True, dtype=torch.float64)
x2 = torch.tensor(20, requires_grad=True, dtype=torch.float64)

# 经过中间的计算
y = x1**2 + x2**2 + x1*x2

# 将输出结果变为标量
y = y.sum()

# 自动微分
y.backward()

# 打印两个变量的梯度
print(x1.grad, x2.grad)


# 多向量梯度计算
# 定义需要计算梯度的张量
x1 = torch.tensor([10, 20], requires_grad=True, dtype=torch.float64)
x2 = torch.tensor([30, 40], requires_grad=True, dtype=torch.float64)

# 经过中间的计算
y = x1 ** 2 + x2 ** 2 + x1 * x2

# 将输出结果变为标量
y = y.sum()

# 自动微分
y.backward()

# 打印两个变量的梯度
print(x1.grad, x2.grad)
```



### 控制梯度计算

```python
import torch

# 控制不计算梯度
x = torch.tensor(10, requires_grad=True, dtype=torch.float64)
print(x.requires_grad)

# 对代码进行装饰
with torch.no_grad():
    y = x ** 2
print(y.requires_grad)

# 对函数进行装饰
@torch.no_grad()
def my_func(x):
    return x ** 2
print(my_func(x).requires_grad)

# set_grad_enabled设置
torch.set_grad_enabled(False)
y = x ** 2
print(y.requires_grad)

# 累计梯度（默认累计，可以通过手动清理梯度）
# 定义需要求导张量
torch.set_grad_enabled(True)
x = torch.tensor([10, 20, 30, 40], requires_grad=True, dtype=torch.float64)

for _ in range(3):
    f1 = x ** 2 + 20
    f2 = f1.sum()

    # 默认张量的 grad 属性会累计历史梯度值，需要每次手动清理上次的梯度
    if x.grad is not None:
        x.grad.data.zero_()

    f2.backward()
    print(x.grad)
    
# 梯度下降优化最优解
x = torch.tensor(10, requires_grad=True, dtype=torch.float64)

for _ in range(5000):
    # 正向计算
    f = x ** 2
    
    # 梯度清零
    if x.grad is not None:
        x.grad.data.zero_()
        
    # 反向传播计算梯度
    f.backward()
    
    # 更新参数
    x.data = x.data - 0.001 * x.grad
    print('%.10f' % x.data)
    
        
```

* **当对设置 requires_grad=True 的张量使用 numpy 函数进行转换时, 会报错，需要先使用 detach 函数将张量进行分离, 再使用 numpy 函数。detach 之后会产生一个新的张量, 新的张量作为叶子结点，并且该张量和原来的张量共享数据。分离后的张量不需要计算梯度。**



## 模型定义（使用pytorch构建线性回归）

* 使用 PyTorch 的 nn.MSELoss() 代替自定义的平方损失函数
* 使用 PyTorch 的 data.DataLoader 代替自定义的数据加载器
* 使用 PyTorch 的 optim.SGD 代替自定义的优化器
* 使用 PyTorch 的 nn.Linear 代替自定义的假设函数

